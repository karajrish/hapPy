[2020-01-20 13:38:18,758] {scheduler_job.py:153} INFO - Started process (PID=4421) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:38:18,765] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:38:18,767] {logging_mixin.py:112} INFO - [2020-01-20 13:38:18,765] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:38:19,077] {logging_mixin.py:112} INFO - [2020-01-20 13:38:19,055] {dagbag.py:246} ERROR - Failed to import: /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
Traceback (most recent call last):
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/site-packages/airflow/models/dagbag.py", line 243, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/imp.py", line 171, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 696, in _load
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py", line 1, in <module>
    from bot.twitter_stream import read_stream_of_tweets
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/bot/twitter_stream.py", line 2, in <module>
    from passwords import *
ModuleNotFoundError: No module named 'passwords'
[2020-01-20 13:38:19,078] {scheduler_job.py:1553} WARNING - No viable dags retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:38:19,098] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.340 seconds
[2020-01-20 13:39:04,890] {scheduler_job.py:153} INFO - Started process (PID=4464) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:39:04,897] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:39:04,898] {logging_mixin.py:112} INFO - [2020-01-20 13:39:04,897] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:39:04,960] {logging_mixin.py:112} INFO - [2020-01-20 13:39:04,955] {dagbag.py:246} ERROR - Failed to import: /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
Traceback (most recent call last):
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/site-packages/airflow/models/dagbag.py", line 243, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/imp.py", line 171, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 696, in _load
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py", line 1, in <module>
    from bot.twitter_stream import read_stream_of_tweets
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/bot/twitter_stream.py", line 3, in <module>
    from kafka_helper import MyKafkaProducer
ModuleNotFoundError: No module named 'kafka_helper'
[2020-01-20 13:39:04,961] {scheduler_job.py:1553} WARNING - No viable dags retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:39:04,974] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.084 seconds
[2020-01-20 13:39:51,036] {scheduler_job.py:153} INFO - Started process (PID=4498) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:39:51,043] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:39:51,044] {logging_mixin.py:112} INFO - [2020-01-20 13:39:51,043] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:39:51,099] {logging_mixin.py:112} INFO - [2020-01-20 13:39:51,096] {dagbag.py:246} ERROR - Failed to import: /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
Traceback (most recent call last):
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/site-packages/airflow/models/dagbag.py", line 243, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/imp.py", line 171, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 696, in _load
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py", line 1, in <module>
    from bot.twitter_stream import read_stream_of_tweets
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/bot/twitter_stream.py", line 3, in <module>
    from kafka_helper import MyKafkaProducer
ModuleNotFoundError: No module named 'kafka_helper'
[2020-01-20 13:39:51,101] {scheduler_job.py:1553} WARNING - No viable dags retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:39:51,115] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.079 seconds
[2020-01-20 13:40:37,152] {scheduler_job.py:153} INFO - Started process (PID=4540) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:40:37,159] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:40:37,161] {logging_mixin.py:112} INFO - [2020-01-20 13:40:37,160] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:40:37,267] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:40:37,277] {logging_mixin.py:112} INFO - [2020-01-20 13:40:37,277] {dag.py:1376} INFO - Creating ORM DAG for stream_from_twitter
[2020-01-20 13:40:37,290] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.138 seconds
[2020-01-20 13:41:23,282] {scheduler_job.py:153} INFO - Started process (PID=4571) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:41:23,290] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:41:23,291] {logging_mixin.py:112} INFO - [2020-01-20 13:41:23,291] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:41:23,621] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:41:23,641] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.359 seconds
[2020-01-20 13:42:09,392] {scheduler_job.py:153} INFO - Started process (PID=4608) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:42:09,407] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:42:09,408] {logging_mixin.py:112} INFO - [2020-01-20 13:42:09,408] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:42:09,734] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:42:09,756] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.364 seconds
[2020-01-20 13:42:55,509] {scheduler_job.py:153} INFO - Started process (PID=4662) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:42:55,520] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:42:55,521] {logging_mixin.py:112} INFO - [2020-01-20 13:42:55,521] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:42:55,821] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:42:55,850] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.341 seconds
[2020-01-20 13:43:41,629] {scheduler_job.py:153} INFO - Started process (PID=4692) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:43:41,634] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:43:41,635] {logging_mixin.py:112} INFO - [2020-01-20 13:43:41,635] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:43:41,721] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:43:41,741] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.112 seconds
[2020-01-20 13:44:27,755] {scheduler_job.py:153} INFO - Started process (PID=4735) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:44:27,770] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:44:27,771] {logging_mixin.py:112} INFO - [2020-01-20 13:44:27,771] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:44:27,893] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:44:27,915] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.159 seconds
[2020-01-20 13:45:13,959] {scheduler_job.py:153} INFO - Started process (PID=4769) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:45:14,065] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:45:14,069] {logging_mixin.py:112} INFO - [2020-01-20 13:45:14,068] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:45:17,472] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:45:17,537] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 3.578 seconds
[2020-01-20 13:46:00,031] {scheduler_job.py:153} INFO - Started process (PID=4824) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:46:00,038] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:46:00,039] {logging_mixin.py:112} INFO - [2020-01-20 13:46:00,039] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:46:00,351] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:46:00,372] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.341 seconds
[2020-01-20 13:46:46,216] {scheduler_job.py:153} INFO - Started process (PID=4864) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:46:46,254] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:46:46,255] {logging_mixin.py:112} INFO - [2020-01-20 13:46:46,255] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:46:46,461] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:46:46,477] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.261 seconds
[2020-01-20 13:47:32,294] {scheduler_job.py:153} INFO - Started process (PID=4947) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:47:32,301] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:47:32,302] {logging_mixin.py:112} INFO - [2020-01-20 13:47:32,302] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:47:32,684] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:47:32,701] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.407 seconds
[2020-01-20 13:48:18,540] {scheduler_job.py:153} INFO - Started process (PID=4991) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:48:18,598] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:48:18,601] {logging_mixin.py:112} INFO - [2020-01-20 13:48:18,601] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:48:18,814] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:48:18,835] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.295 seconds
[2020-01-20 13:49:04,566] {scheduler_job.py:153} INFO - Started process (PID=5022) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:49:04,574] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:49:04,575] {logging_mixin.py:112} INFO - [2020-01-20 13:49:04,574] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:49:04,861] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:49:04,879] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.312 seconds
[2020-01-20 13:49:50,704] {scheduler_job.py:153} INFO - Started process (PID=5061) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:49:50,710] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:49:50,711] {logging_mixin.py:112} INFO - [2020-01-20 13:49:50,710] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:49:50,790] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:49:50,807] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.103 seconds
[2020-01-20 13:50:36,857] {scheduler_job.py:153} INFO - Started process (PID=5090) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:50:36,864] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:50:36,864] {logging_mixin.py:112} INFO - [2020-01-20 13:50:36,864] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:50:37,023] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:50:37,039] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.183 seconds
[2020-01-20 13:51:22,976] {scheduler_job.py:153} INFO - Started process (PID=5123) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:51:22,985] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:51:22,985] {logging_mixin.py:112} INFO - [2020-01-20 13:51:22,985] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:51:23,195] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:51:23,212] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.236 seconds
[2020-01-20 13:52:09,103] {scheduler_job.py:153} INFO - Started process (PID=5152) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:52:09,109] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:52:09,110] {logging_mixin.py:112} INFO - [2020-01-20 13:52:09,109] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:52:09,203] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:52:09,220] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.118 seconds
[2020-01-20 13:52:55,205] {scheduler_job.py:153} INFO - Started process (PID=5187) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:52:55,212] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:52:55,213] {logging_mixin.py:112} INFO - [2020-01-20 13:52:55,212] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:52:55,311] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:52:55,327] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.122 seconds
[2020-01-20 13:53:41,358] {scheduler_job.py:153} INFO - Started process (PID=5217) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:53:41,365] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:53:41,366] {logging_mixin.py:112} INFO - [2020-01-20 13:53:41,366] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:53:41,450] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:53:41,465] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.108 seconds
[2020-01-20 13:57:28,992] {scheduler_job.py:153} INFO - Started process (PID=5284) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:57:28,997] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:57:28,999] {logging_mixin.py:112} INFO - [2020-01-20 13:57:28,998] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:57:29,132] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:57:29,150] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 13:57:29,175] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2019-10-09T00:00:00+00:00: scheduled__2019-10-09T00:00:00+00:00, externally triggered: False>
[2020-01-20 13:57:29,179] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 00:00:00+00:00: scheduled__2019-10-09T00:00:00+00:00, externally triggered: False>
[2020-01-20 13:57:29,189] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 13:57:29,193] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 00:00:00+00:00 [scheduled]> in ORM
[2020-01-20 13:57:29,199] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.207 seconds
[2020-01-20 13:58:40,624] {scheduler_job.py:153} INFO - Started process (PID=5332) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:58:40,637] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:58:40,638] {logging_mixin.py:112} INFO - [2020-01-20 13:58:40,638] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:58:40,862] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:58:40,912] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 13:58:40,944] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2019-10-09T01:00:00+00:00: scheduled__2019-10-09T01:00:00+00:00, externally triggered: False>
[2020-01-20 13:58:40,946] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 00:00:00+00:00: scheduled__2019-10-09T00:00:00+00:00, externally triggered: False>
[2020-01-20 13:58:40,955] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 01:00:00+00:00: scheduled__2019-10-09T01:00:00+00:00, externally triggered: False>
[2020-01-20 13:58:40,969] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 13:58:40,972] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 01:00:00+00:00 [scheduled]> in ORM
[2020-01-20 13:58:40,992] {logging_mixin.py:112} INFO - [2020-01-20 13:58:40,991] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 00:00:00+00:00 [failed]> as failed
[2020-01-20 13:58:40,992] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.368 seconds
[2020-01-20 13:59:51,976] {scheduler_job.py:153} INFO - Started process (PID=5384) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:59:51,987] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 13:59:51,988] {logging_mixin.py:112} INFO - [2020-01-20 13:59:51,987] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:59:52,158] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 13:59:52,186] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 13:59:52,208] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2019-10-09T02:00:00+00:00: scheduled__2019-10-09T02:00:00+00:00, externally triggered: False>
[2020-01-20 13:59:52,212] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 00:00:00+00:00: scheduled__2019-10-09T00:00:00+00:00, externally triggered: False>
[2020-01-20 13:59:52,218] {logging_mixin.py:112} INFO - [2020-01-20 13:59:52,218] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2019-10-09 00:00:00+00:00: scheduled__2019-10-09T00:00:00+00:00, externally triggered: False> failed
[2020-01-20 13:59:52,221] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 01:00:00+00:00: scheduled__2019-10-09T01:00:00+00:00, externally triggered: False>
[2020-01-20 13:59:52,232] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 02:00:00+00:00: scheduled__2019-10-09T02:00:00+00:00, externally triggered: False>
[2020-01-20 13:59:52,243] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 13:59:52,247] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 02:00:00+00:00 [scheduled]> in ORM
[2020-01-20 13:59:52,266] {logging_mixin.py:112} INFO - [2020-01-20 13:59:52,265] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 01:00:00+00:00 [failed]> as failed
[2020-01-20 13:59:52,267] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.291 seconds
[2020-01-20 14:01:02,320] {scheduler_job.py:153} INFO - Started process (PID=5429) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:01:02,327] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:01:02,328] {logging_mixin.py:112} INFO - [2020-01-20 14:01:02,328] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:01:02,532] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:01:02,556] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:01:02,579] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2019-10-09T03:00:00+00:00: scheduled__2019-10-09T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:01:02,583] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 01:00:00+00:00: scheduled__2019-10-09T01:00:00+00:00, externally triggered: False>
[2020-01-20 14:01:02,589] {logging_mixin.py:112} INFO - [2020-01-20 14:01:02,589] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2019-10-09 01:00:00+00:00: scheduled__2019-10-09T01:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:01:02,592] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 02:00:00+00:00: scheduled__2019-10-09T02:00:00+00:00, externally triggered: False>
[2020-01-20 14:01:02,600] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 03:00:00+00:00: scheduled__2019-10-09T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:01:02,613] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:01:02,617] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 03:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:01:02,639] {logging_mixin.py:112} INFO - [2020-01-20 14:01:02,638] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 02:00:00+00:00 [failed]> as failed
[2020-01-20 14:01:02,640] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.320 seconds
[2020-01-20 14:02:14,642] {scheduler_job.py:153} INFO - Started process (PID=5472) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:02:14,649] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:02:14,651] {logging_mixin.py:112} INFO - [2020-01-20 14:02:14,650] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:02:14,741] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:02:14,763] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:02:14,791] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2019-10-09T04:00:00+00:00: scheduled__2019-10-09T04:00:00+00:00, externally triggered: False>
[2020-01-20 14:02:14,794] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 02:00:00+00:00: scheduled__2019-10-09T02:00:00+00:00, externally triggered: False>
[2020-01-20 14:02:14,801] {logging_mixin.py:112} INFO - [2020-01-20 14:02:14,801] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2019-10-09 02:00:00+00:00: scheduled__2019-10-09T02:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:02:14,804] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 03:00:00+00:00: scheduled__2019-10-09T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:02:14,812] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 04:00:00+00:00: scheduled__2019-10-09T04:00:00+00:00, externally triggered: False>
[2020-01-20 14:02:14,826] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:02:14,829] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 04:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:02:14,854] {logging_mixin.py:112} INFO - [2020-01-20 14:02:14,853] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 03:00:00+00:00 [failed]> as failed
[2020-01-20 14:02:14,854] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.212 seconds
[2020-01-20 14:03:24,693] {scheduler_job.py:153} INFO - Started process (PID=5531) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:03:24,702] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:03:24,708] {logging_mixin.py:112} INFO - [2020-01-20 14:03:24,707] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:03:24,944] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:03:24,967] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:03:24,992] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2019-10-09T05:00:00+00:00: scheduled__2019-10-09T05:00:00+00:00, externally triggered: False>
[2020-01-20 14:03:24,995] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 03:00:00+00:00: scheduled__2019-10-09T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:03:25,001] {logging_mixin.py:112} INFO - [2020-01-20 14:03:25,001] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2019-10-09 03:00:00+00:00: scheduled__2019-10-09T03:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:03:25,004] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 04:00:00+00:00: scheduled__2019-10-09T04:00:00+00:00, externally triggered: False>
[2020-01-20 14:03:25,011] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2019-10-09 05:00:00+00:00: scheduled__2019-10-09T05:00:00+00:00, externally triggered: False>
[2020-01-20 14:03:25,025] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:03:25,029] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 05:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:03:25,046] {logging_mixin.py:112} INFO - [2020-01-20 14:03:25,046] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2019-10-09 04:00:00+00:00 [failed]> as failed
[2020-01-20 14:03:25,047] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.353 seconds
[2020-01-20 14:10:05,426] {scheduler_job.py:153} INFO - Started process (PID=5629) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:10:05,435] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:10:05,436] {logging_mixin.py:112} INFO - [2020-01-20 14:10:05,435] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:10:05,509] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:10:05,529] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:10:05,556] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T00:00:00+00:00: scheduled__2020-01-20T00:00:00+00:00, externally triggered: False>
[2020-01-20 14:10:05,559] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 00:00:00+00:00: scheduled__2020-01-20T00:00:00+00:00, externally triggered: False>
[2020-01-20 14:10:05,568] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:10:05,571] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 00:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:10:05,578] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.152 seconds
[2020-01-20 14:11:18,816] {scheduler_job.py:153} INFO - Started process (PID=5674) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:11:18,824] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:11:18,826] {logging_mixin.py:112} INFO - [2020-01-20 14:11:18,825] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:11:19,032] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:11:19,052] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:11:19,077] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T01:00:00+00:00: scheduled__2020-01-20T01:00:00+00:00, externally triggered: False>
[2020-01-20 14:11:19,081] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 00:00:00+00:00: scheduled__2020-01-20T00:00:00+00:00, externally triggered: False>
[2020-01-20 14:11:19,089] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 01:00:00+00:00: scheduled__2020-01-20T01:00:00+00:00, externally triggered: False>
[2020-01-20 14:11:19,102] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:11:19,106] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 01:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:11:19,125] {logging_mixin.py:112} INFO - [2020-01-20 14:11:19,124] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 00:00:00+00:00 [failed]> as failed
[2020-01-20 14:11:19,125] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.309 seconds
[2020-01-20 14:12:27,773] {scheduler_job.py:153} INFO - Started process (PID=5711) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:12:27,779] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:12:27,780] {logging_mixin.py:112} INFO - [2020-01-20 14:12:27,780] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:12:27,855] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:12:27,874] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:12:27,896] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T02:00:00+00:00: scheduled__2020-01-20T02:00:00+00:00, externally triggered: False>
[2020-01-20 14:12:27,899] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 00:00:00+00:00: scheduled__2020-01-20T00:00:00+00:00, externally triggered: False>
[2020-01-20 14:12:27,904] {logging_mixin.py:112} INFO - [2020-01-20 14:12:27,904] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 00:00:00+00:00: scheduled__2020-01-20T00:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:12:27,907] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 01:00:00+00:00: scheduled__2020-01-20T01:00:00+00:00, externally triggered: False>
[2020-01-20 14:12:27,913] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 02:00:00+00:00: scheduled__2020-01-20T02:00:00+00:00, externally triggered: False>
[2020-01-20 14:12:27,924] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:12:27,927] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 02:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:12:27,945] {logging_mixin.py:112} INFO - [2020-01-20 14:12:27,944] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 01:00:00+00:00 [failed]> as failed
[2020-01-20 14:12:27,945] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.172 seconds
[2020-01-20 14:13:36,907] {scheduler_job.py:153} INFO - Started process (PID=5746) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:13:36,913] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:13:36,914] {logging_mixin.py:112} INFO - [2020-01-20 14:13:36,913] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:13:36,987] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:13:37,010] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:13:37,035] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T03:00:00+00:00: scheduled__2020-01-20T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:13:37,038] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 01:00:00+00:00: scheduled__2020-01-20T01:00:00+00:00, externally triggered: False>
[2020-01-20 14:13:37,044] {logging_mixin.py:112} INFO - [2020-01-20 14:13:37,044] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 01:00:00+00:00: scheduled__2020-01-20T01:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:13:37,047] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 02:00:00+00:00: scheduled__2020-01-20T02:00:00+00:00, externally triggered: False>
[2020-01-20 14:13:37,054] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 03:00:00+00:00: scheduled__2020-01-20T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:13:37,065] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:13:37,068] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 03:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:13:37,087] {logging_mixin.py:112} INFO - [2020-01-20 14:13:37,087] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 02:00:00+00:00 [failed]> as failed
[2020-01-20 14:13:37,088] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.181 seconds
[2020-01-20 14:14:45,807] {scheduler_job.py:153} INFO - Started process (PID=5783) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:14:45,815] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:14:45,816] {logging_mixin.py:112} INFO - [2020-01-20 14:14:45,815] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:14:45,891] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:14:45,912] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:14:45,938] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T04:00:00+00:00: scheduled__2020-01-20T04:00:00+00:00, externally triggered: False>
[2020-01-20 14:14:45,940] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 02:00:00+00:00: scheduled__2020-01-20T02:00:00+00:00, externally triggered: False>
[2020-01-20 14:14:45,946] {logging_mixin.py:112} INFO - [2020-01-20 14:14:45,946] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 02:00:00+00:00: scheduled__2020-01-20T02:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:14:45,949] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 03:00:00+00:00: scheduled__2020-01-20T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:14:45,957] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 04:00:00+00:00: scheduled__2020-01-20T04:00:00+00:00, externally triggered: False>
[2020-01-20 14:14:45,967] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:14:45,970] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 04:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:14:45,988] {logging_mixin.py:112} INFO - [2020-01-20 14:14:45,988] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 03:00:00+00:00 [failed]> as failed
[2020-01-20 14:14:45,989] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.182 seconds
[2020-01-20 14:15:53,286] {scheduler_job.py:153} INFO - Started process (PID=5818) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:15:53,292] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:15:53,292] {logging_mixin.py:112} INFO - [2020-01-20 14:15:53,292] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:15:53,365] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:15:53,386] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:15:53,408] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T05:00:00+00:00: scheduled__2020-01-20T05:00:00+00:00, externally triggered: False>
[2020-01-20 14:15:53,411] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 03:00:00+00:00: scheduled__2020-01-20T03:00:00+00:00, externally triggered: False>
[2020-01-20 14:15:53,416] {logging_mixin.py:112} INFO - [2020-01-20 14:15:53,416] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 03:00:00+00:00: scheduled__2020-01-20T03:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:15:53,419] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 04:00:00+00:00: scheduled__2020-01-20T04:00:00+00:00, externally triggered: False>
[2020-01-20 14:15:53,425] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 05:00:00+00:00: scheduled__2020-01-20T05:00:00+00:00, externally triggered: False>
[2020-01-20 14:15:53,436] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:15:53,439] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 05:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:15:53,457] {logging_mixin.py:112} INFO - [2020-01-20 14:15:53,456] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 04:00:00+00:00 [failed]> as failed
[2020-01-20 14:15:53,457] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.171 seconds
[2020-01-20 14:17:02,151] {scheduler_job.py:153} INFO - Started process (PID=5851) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:17:02,158] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:17:02,159] {logging_mixin.py:112} INFO - [2020-01-20 14:17:02,159] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:17:02,231] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:17:02,255] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:17:02,279] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T06:00:00+00:00: scheduled__2020-01-20T06:00:00+00:00, externally triggered: False>
[2020-01-20 14:17:02,281] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 04:00:00+00:00: scheduled__2020-01-20T04:00:00+00:00, externally triggered: False>
[2020-01-20 14:17:02,287] {logging_mixin.py:112} INFO - [2020-01-20 14:17:02,286] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 04:00:00+00:00: scheduled__2020-01-20T04:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:17:02,289] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 05:00:00+00:00: scheduled__2020-01-20T05:00:00+00:00, externally triggered: False>
[2020-01-20 14:17:02,296] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 06:00:00+00:00: scheduled__2020-01-20T06:00:00+00:00, externally triggered: False>
[2020-01-20 14:17:02,306] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:17:02,309] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 06:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:17:02,327] {logging_mixin.py:112} INFO - [2020-01-20 14:17:02,327] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 05:00:00+00:00 [failed]> as failed
[2020-01-20 14:17:02,328] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.177 seconds
[2020-01-20 14:18:10,702] {scheduler_job.py:153} INFO - Started process (PID=5889) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:18:10,710] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:18:10,711] {logging_mixin.py:112} INFO - [2020-01-20 14:18:10,711] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:18:10,782] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:18:10,804] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:18:10,826] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T07:00:00+00:00: scheduled__2020-01-20T07:00:00+00:00, externally triggered: False>
[2020-01-20 14:18:10,829] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 05:00:00+00:00: scheduled__2020-01-20T05:00:00+00:00, externally triggered: False>
[2020-01-20 14:18:10,836] {logging_mixin.py:112} INFO - [2020-01-20 14:18:10,835] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 05:00:00+00:00: scheduled__2020-01-20T05:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:18:10,838] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 06:00:00+00:00: scheduled__2020-01-20T06:00:00+00:00, externally triggered: False>
[2020-01-20 14:18:10,845] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 07:00:00+00:00: scheduled__2020-01-20T07:00:00+00:00, externally triggered: False>
[2020-01-20 14:18:10,856] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:18:10,859] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 07:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:18:10,880] {logging_mixin.py:112} INFO - [2020-01-20 14:18:10,879] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 06:00:00+00:00 [failed]> as failed
[2020-01-20 14:18:10,880] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.178 seconds
[2020-01-20 14:19:18,702] {scheduler_job.py:153} INFO - Started process (PID=5924) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:19:18,711] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:19:18,712] {logging_mixin.py:112} INFO - [2020-01-20 14:19:18,712] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:19:18,785] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:19:18,807] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:19:18,817] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 06:00:00+00:00: scheduled__2020-01-20T06:00:00+00:00, externally triggered: False>
[2020-01-20 14:19:18,824] {logging_mixin.py:112} INFO - [2020-01-20 14:19:18,823] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 06:00:00+00:00: scheduled__2020-01-20T06:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:19:18,827] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 07:00:00+00:00: scheduled__2020-01-20T07:00:00+00:00, externally triggered: False>
[2020-01-20 14:19:18,836] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:19:18,851] {logging_mixin.py:112} INFO - [2020-01-20 14:19:18,851] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 07:00:00+00:00 [failed]> as failed
[2020-01-20 14:19:18,851] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.150 seconds
[2020-01-20 14:20:15,642] {scheduler_job.py:153} INFO - Started process (PID=5957) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:20:15,649] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:20:15,650] {logging_mixin.py:112} INFO - [2020-01-20 14:20:15,650] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:20:15,941] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:20:15,965] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:20:15,975] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 07:00:00+00:00: scheduled__2020-01-20T07:00:00+00:00, externally triggered: False>
[2020-01-20 14:20:15,983] {logging_mixin.py:112} INFO - [2020-01-20 14:20:15,983] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 07:00:00+00:00: scheduled__2020-01-20T07:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:20:15,986] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:20:15,989] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.347 seconds
[2020-01-20 14:21:13,794] {scheduler_job.py:153} INFO - Started process (PID=5987) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:21:13,799] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:21:13,799] {logging_mixin.py:112} INFO - [2020-01-20 14:21:13,799] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:21:13,872] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:21:13,893] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:21:13,903] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:21:13,906] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.112 seconds
[2020-01-20 14:22:10,434] {scheduler_job.py:153} INFO - Started process (PID=6027) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:22:10,441] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:22:10,442] {logging_mixin.py:112} INFO - [2020-01-20 14:22:10,442] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:22:10,517] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:22:10,541] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:22:10,551] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:22:10,554] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.120 seconds
[2020-01-20 14:23:08,260] {scheduler_job.py:153} INFO - Started process (PID=6061) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:23:08,269] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:23:08,270] {logging_mixin.py:112} INFO - [2020-01-20 14:23:08,269] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:23:08,342] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:23:08,367] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:23:08,377] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:23:08,380] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.120 seconds
[2020-01-20 14:24:07,946] {scheduler_job.py:153} INFO - Started process (PID=6114) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:24:07,955] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:24:07,956] {logging_mixin.py:112} INFO - [2020-01-20 14:24:07,955] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:24:08,040] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:24:08,060] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:24:08,071] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:24:08,074] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.128 seconds
[2020-01-20 14:25:06,671] {scheduler_job.py:153} INFO - Started process (PID=6145) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:25:06,679] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:25:06,681] {logging_mixin.py:112} INFO - [2020-01-20 14:25:06,680] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:25:06,927] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:25:06,953] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:25:06,965] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:25:06,968] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.297 seconds
[2020-01-20 14:26:05,615] {scheduler_job.py:153} INFO - Started process (PID=6177) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:26:05,624] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:26:05,625] {logging_mixin.py:112} INFO - [2020-01-20 14:26:05,625] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:26:05,847] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:26:05,867] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:26:05,879] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:26:05,882] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.267 seconds
[2020-01-20 14:27:05,032] {scheduler_job.py:153} INFO - Started process (PID=6210) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:27:05,039] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:27:05,040] {logging_mixin.py:112} INFO - [2020-01-20 14:27:05,039] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:27:05,151] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:27:05,170] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:27:05,180] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:27:05,183] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.151 seconds
[2020-01-20 14:28:03,104] {scheduler_job.py:153} INFO - Started process (PID=6245) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:28:03,110] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:28:03,111] {logging_mixin.py:112} INFO - [2020-01-20 14:28:03,111] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:28:03,306] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:28:03,326] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:28:03,336] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:28:03,338] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.235 seconds
[2020-01-20 14:29:00,803] {scheduler_job.py:153} INFO - Started process (PID=6282) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:29:00,809] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:29:00,809] {logging_mixin.py:112} INFO - [2020-01-20 14:29:00,809] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:29:00,909] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:29:00,930] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:29:00,944] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:29:00,947] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.145 seconds
[2020-01-20 14:29:59,282] {scheduler_job.py:153} INFO - Started process (PID=6312) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:29:59,289] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:29:59,291] {logging_mixin.py:112} INFO - [2020-01-20 14:29:59,290] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:29:59,407] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:29:59,430] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:29:59,443] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:29:59,446] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.165 seconds
[2020-01-20 14:30:57,638] {scheduler_job.py:153} INFO - Started process (PID=6345) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:30:57,645] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:30:57,646] {logging_mixin.py:112} INFO - [2020-01-20 14:30:57,646] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:30:57,834] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:30:57,858] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:30:57,894] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False>
[2020-01-20 14:30:57,898] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False>
[2020-01-20 14:30:57,912] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:30:57,917] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 08:00:00+00:00 [scheduled]> in ORM
[2020-01-20 14:30:57,924] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.286 seconds
[2020-01-20 14:32:07,237] {scheduler_job.py:153} INFO - Started process (PID=6382) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:32:07,245] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:32:07,246] {logging_mixin.py:112} INFO - [2020-01-20 14:32:07,246] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:32:07,324] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:32:07,343] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:32:07,354] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False>
[2020-01-20 14:32:07,364] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:32:07,387] {logging_mixin.py:112} INFO - [2020-01-20 14:32:07,387] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 08:00:00+00:00 [failed]> as failed
[2020-01-20 14:32:07,388] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.151 seconds
[2020-01-20 14:33:07,248] {scheduler_job.py:153} INFO - Started process (PID=6429) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:33:07,257] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:33:07,258] {logging_mixin.py:112} INFO - [2020-01-20 14:33:07,258] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:33:07,548] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:33:07,571] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:33:07,581] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False>
[2020-01-20 14:33:07,589] {logging_mixin.py:112} INFO - [2020-01-20 14:33:07,589] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:33:07,592] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:33:07,595] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.347 seconds
[2020-01-20 14:34:05,419] {scheduler_job.py:153} INFO - Started process (PID=6468) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:34:05,427] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:34:05,427] {logging_mixin.py:112} INFO - [2020-01-20 14:34:05,427] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:34:05,499] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:34:05,521] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:34:05,530] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:34:05,533] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.114 seconds
[2020-01-20 14:35:03,244] {scheduler_job.py:153} INFO - Started process (PID=6505) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:35:03,253] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:35:03,254] {logging_mixin.py:112} INFO - [2020-01-20 14:35:03,254] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:35:03,339] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:35:03,359] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:35:03,371] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:35:03,373] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.129 seconds
[2020-01-20 14:41:14,620] {scheduler_job.py:153} INFO - Started process (PID=6585) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:41:14,631] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:41:14,631] {logging_mixin.py:112} INFO - [2020-01-20 14:41:14,631] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:41:14,888] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:41:14,912] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:41:14,939] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False>
[2020-01-20 14:41:14,941] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False>
[2020-01-20 14:41:14,949] {logging_mixin.py:112} INFO - [2020-01-20 14:41:14,949] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 08:00:00+00:00: scheduled__2020-01-20T08:00:00+00:00, externally triggered: False> failed
[2020-01-20 14:41:14,952] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:41:14,954] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.334 seconds
[2020-01-20 14:42:00,738] {scheduler_job.py:153} INFO - Started process (PID=6611) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:42:00,745] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:42:00,746] {logging_mixin.py:112} INFO - [2020-01-20 14:42:00,745] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:42:00,820] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:42:00,841] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:42:00,854] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:42:00,856] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.118 seconds
[2020-01-20 14:42:46,858] {scheduler_job.py:153} INFO - Started process (PID=6650) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:42:46,865] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:42:46,866] {logging_mixin.py:112} INFO - [2020-01-20 14:42:46,866] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:42:46,958] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:42:46,981] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:42:46,993] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:42:46,996] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.139 seconds
[2020-01-20 14:43:33,015] {scheduler_job.py:153} INFO - Started process (PID=6684) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:43:33,023] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:43:33,024] {logging_mixin.py:112} INFO - [2020-01-20 14:43:33,024] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:43:33,101] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:43:33,122] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:43:33,134] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:43:33,137] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.122 seconds
[2020-01-20 14:44:19,137] {scheduler_job.py:153} INFO - Started process (PID=6710) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:44:19,143] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:44:19,144] {logging_mixin.py:112} INFO - [2020-01-20 14:44:19,143] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:44:19,220] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:44:19,240] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:44:19,252] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:44:19,255] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.118 seconds
[2020-01-20 14:45:05,269] {scheduler_job.py:153} INFO - Started process (PID=6739) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:45:05,276] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:45:05,278] {logging_mixin.py:112} INFO - [2020-01-20 14:45:05,277] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:45:05,486] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:45:05,506] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:45:05,519] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:14:57.035160+00:00: manual__2020-01-20T09:14:57.035160+00:00, externally triggered: True>
[2020-01-20 14:45:05,529] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:45:05,532] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:14:57.035160+00:00 [scheduled]> in ORM
[2020-01-20 14:45:05,540] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.271 seconds
[2020-01-20 14:46:04,704] {scheduler_job.py:153} INFO - Started process (PID=6777) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:46:04,716] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:46:04,718] {logging_mixin.py:112} INFO - [2020-01-20 14:46:04,717] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:46:04,963] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:46:04,983] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:46:04,996] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:14:57.035160+00:00: manual__2020-01-20T09:14:57.035160+00:00, externally triggered: True>
[2020-01-20 14:46:05,007] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:46:05,023] {logging_mixin.py:112} INFO - [2020-01-20 14:46:05,022] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:14:57.035160+00:00 [failed]> as failed
[2020-01-20 14:46:05,023] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.320 seconds
[2020-01-20 14:46:50,838] {scheduler_job.py:153} INFO - Started process (PID=6801) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:46:50,847] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:46:50,848] {logging_mixin.py:112} INFO - [2020-01-20 14:46:50,847] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:46:51,182] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:46:51,465] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:46:51,596] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:14:57.035160+00:00: manual__2020-01-20T09:14:57.035160+00:00, externally triggered: True>
[2020-01-20 14:46:51,660] {logging_mixin.py:112} INFO - [2020-01-20 14:46:51,658] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:14:57.035160+00:00: manual__2020-01-20T09:14:57.035160+00:00, externally triggered: True> failed
[2020-01-20 14:46:51,675] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:46:51,697] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.858 seconds
[2020-01-20 14:47:36,960] {scheduler_job.py:153} INFO - Started process (PID=6839) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:47:36,969] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:47:36,969] {logging_mixin.py:112} INFO - [2020-01-20 14:47:36,969] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:47:37,076] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:47:37,103] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:47:37,122] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:47:37,125] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.166 seconds
[2020-01-20 14:48:23,088] {scheduler_job.py:153} INFO - Started process (PID=6864) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:48:23,094] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:48:23,095] {logging_mixin.py:112} INFO - [2020-01-20 14:48:23,094] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:48:23,172] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:48:23,196] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:48:23,210] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:48:23,214] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.127 seconds
[2020-01-20 14:49:09,192] {scheduler_job.py:153} INFO - Started process (PID=6894) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:49:09,199] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:49:09,199] {logging_mixin.py:112} INFO - [2020-01-20 14:49:09,199] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:49:09,441] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:49:09,467] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:49:09,481] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:49:09,484] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.292 seconds
[2020-01-20 14:49:55,328] {scheduler_job.py:153} INFO - Started process (PID=6921) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:49:55,336] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:49:55,337] {logging_mixin.py:112} INFO - [2020-01-20 14:49:55,337] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:49:55,410] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:49:55,433] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:49:55,444] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:49:55,447] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.119 seconds
[2020-01-20 14:50:41,453] {scheduler_job.py:153} INFO - Started process (PID=6950) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:50:41,466] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:50:41,467] {logging_mixin.py:112} INFO - [2020-01-20 14:50:41,466] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:50:41,546] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:50:41,570] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:50:41,582] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:50:41,585] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.132 seconds
[2020-01-20 14:51:27,581] {scheduler_job.py:153} INFO - Started process (PID=6980) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:51:27,588] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:51:27,590] {logging_mixin.py:112} INFO - [2020-01-20 14:51:27,589] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:51:27,693] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:51:27,719] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:51:27,732] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:51:27,735] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.155 seconds
[2020-01-20 14:52:13,699] {scheduler_job.py:153} INFO - Started process (PID=7011) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:52:13,706] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:52:13,707] {logging_mixin.py:112} INFO - [2020-01-20 14:52:13,707] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:52:13,790] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:52:13,812] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:52:13,826] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:52:13,829] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.130 seconds
[2020-01-20 14:52:59,815] {scheduler_job.py:153} INFO - Started process (PID=7042) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:52:59,830] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:52:59,834] {logging_mixin.py:112} INFO - [2020-01-20 14:52:59,831] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:53:00,477] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:53:00,518] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:53:00,537] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:53:00,540] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.725 seconds
[2020-01-20 14:53:45,951] {scheduler_job.py:153} INFO - Started process (PID=7075) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:53:45,958] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:53:45,960] {logging_mixin.py:112} INFO - [2020-01-20 14:53:45,959] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:53:46,230] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:53:46,270] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:53:46,293] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:53:46,298] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.347 seconds
[2020-01-20 14:54:32,096] {scheduler_job.py:153} INFO - Started process (PID=7115) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:54:32,104] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:54:32,105] {logging_mixin.py:112} INFO - [2020-01-20 14:54:32,105] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:54:32,328] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:54:32,352] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:54:32,372] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:54:32,376] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.280 seconds
[2020-01-20 14:55:18,227] {scheduler_job.py:153} INFO - Started process (PID=7140) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:55:18,253] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:55:18,254] {logging_mixin.py:112} INFO - [2020-01-20 14:55:18,253] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:55:18,406] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:55:18,430] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:55:18,442] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:55:18,445] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.218 seconds
[2020-01-20 14:56:04,356] {scheduler_job.py:153} INFO - Started process (PID=7164) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:56:04,363] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:56:04,364] {logging_mixin.py:112} INFO - [2020-01-20 14:56:04,364] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:56:04,668] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:56:04,690] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:56:04,702] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:56:04,704] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.348 seconds
[2020-01-20 14:56:50,495] {scheduler_job.py:153} INFO - Started process (PID=7189) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:56:50,503] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:56:50,504] {logging_mixin.py:112} INFO - [2020-01-20 14:56:50,504] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:56:50,588] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:56:50,608] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:56:50,620] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:56:50,623] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.128 seconds
[2020-01-20 14:57:36,630] {scheduler_job.py:153} INFO - Started process (PID=7226) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:57:36,639] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:57:36,640] {logging_mixin.py:112} INFO - [2020-01-20 14:57:36,640] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:57:36,936] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:57:36,962] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:57:36,976] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:57:36,979] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.349 seconds
[2020-01-20 14:58:22,736] {scheduler_job.py:153} INFO - Started process (PID=7257) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:58:22,743] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:58:22,743] {logging_mixin.py:112} INFO - [2020-01-20 14:58:22,743] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:58:22,999] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:58:23,022] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:58:23,039] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:27:55+00:00: manual__2020-01-20T09:27:55+00:00, externally triggered: True>
[2020-01-20 14:58:23,057] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:58:23,062] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:27:55+00:00 [scheduled]> in ORM
[2020-01-20 14:58:23,073] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.337 seconds
[2020-01-20 14:59:21,695] {scheduler_job.py:153} INFO - Started process (PID=7293) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:59:21,703] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 14:59:21,704] {logging_mixin.py:112} INFO - [2020-01-20 14:59:21,704] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:59:21,780] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 14:59:21,803] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 14:59:21,815] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:27:55+00:00: manual__2020-01-20T09:27:55+00:00, externally triggered: True>
[2020-01-20 14:59:21,825] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 14:59:21,845] {logging_mixin.py:112} INFO - [2020-01-20 14:59:21,845] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:27:55+00:00 [failed]> as failed
[2020-01-20 14:59:21,846] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.151 seconds
[2020-01-20 15:00:07,820] {scheduler_job.py:153} INFO - Started process (PID=7323) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:00:07,826] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:00:07,827] {logging_mixin.py:112} INFO - [2020-01-20 15:00:07,827] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:00:07,910] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:00:07,936] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:00:07,948] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:27:55+00:00: manual__2020-01-20T09:27:55+00:00, externally triggered: True>
[2020-01-20 15:00:07,957] {logging_mixin.py:112} INFO - [2020-01-20 15:00:07,956] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:27:55+00:00: manual__2020-01-20T09:27:55+00:00, externally triggered: True> failed
[2020-01-20 15:00:07,960] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:00:07,962] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.143 seconds
[2020-01-20 15:01:03,860] {logging_mixin.py:112} INFO - [2020-01-20 15:01:03,857] {settings.py:213} DEBUG - Setting up DB connection pool (PID 7368)
[2020-01-20 15:01:03,868] {scheduler_job.py:153} INFO - Started process (PID=7368) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:03,879] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:01:03,880] {logging_mixin.py:112} INFO - [2020-01-20 15:01:03,879] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:03,893] {logging_mixin.py:112} INFO - [2020-01-20 15:01:03,893] {dagbag.py:232} DEBUG - Importing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:04,101] {logging_mixin.py:112} INFO - [2020-01-20 15:01:04,100] {dagbag.py:370} DEBUG - Loaded DAG <DAG: stream_from_twitter>
[2020-01-20 15:01:04,102] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:04,144] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:01:04,173] {scheduler_job.py:691} DEBUG - Dag start date: 2020-01-20T08:00:00+00:00. Next run date: 2020-01-20T09:00:00+00:00
[2020-01-20 15:01:04,180] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:01:04,196] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.329 seconds
[2020-01-20 15:01:50,008] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,008] {settings.py:213} DEBUG - Setting up DB connection pool (PID 7396)
[2020-01-20 15:01:50,013] {scheduler_job.py:153} INFO - Started process (PID=7396) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:50,020] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:01:50,021] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,021] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:50,022] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,022] {dagbag.py:232} DEBUG - Importing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:50,102] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,100] {dagbag.py:370} DEBUG - Loaded DAG <DAG: stream_from_twitter>
[2020-01-20 15:01:50,102] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:01:50,123] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:01:50,132] {scheduler_job.py:691} DEBUG - Dag start date: 2020-01-20T08:00:00+00:00. Next run date: 2020-01-20T09:00:00+00:00
[2020-01-20 15:01:50,135] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True>
[2020-01-20 15:01:50,140] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,139] {dagrun.py:263} DEBUG - Updating state for <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True> considering 1 task(s)
[2020-01-20 15:01:50,142] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,142] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The context specified that being in a retry period was permitted.
[2020-01-20 15:01:50,143] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,142] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2020-01-20 15:01:50,143] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,143] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2020-01-20 15:01:50,143] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,143] {taskinstance.py:655} DEBUG - Dependencies all met for <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]>
[2020-01-20 15:01:50,144] {scheduler_job.py:767} DEBUG - Examining active DAG run: <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True>
[2020-01-20 15:01:50,147] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,147] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2020-01-20 15:01:50,147] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,147] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2020-01-20 15:01:50,148] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,148] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2020-01-20 15:01:50,148] {logging_mixin.py:112} INFO - [2020-01-20 15:01:50,148] {taskinstance.py:655} DEBUG - Dependencies all met for <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]>
[2020-01-20 15:01:50,148] {scheduler_job.py:782} DEBUG - Queuing task: <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [None]>
[2020-01-20 15:01:50,149] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:01:50,152] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [scheduled]> in ORM
[2020-01-20 15:01:50,158] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.146 seconds
[2020-01-20 15:02:48,583] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,582] {settings.py:213} DEBUG - Setting up DB connection pool (PID 7436)
[2020-01-20 15:02:48,590] {scheduler_job.py:153} INFO - Started process (PID=7436) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:02:48,597] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:02:48,598] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,597] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:02:48,599] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,599] {dagbag.py:232} DEBUG - Importing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:02:48,694] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,693] {dagbag.py:370} DEBUG - Loaded DAG <DAG: stream_from_twitter>
[2020-01-20 15:02:48,695] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:02:48,714] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:02:48,723] {scheduler_job.py:691} DEBUG - Dag start date: 2020-01-20T08:00:00+00:00. Next run date: 2020-01-20T09:00:00+00:00
[2020-01-20 15:02:48,725] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True>
[2020-01-20 15:02:48,730] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,730] {dagrun.py:263} DEBUG - Updating state for <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True> considering 1 task(s)
[2020-01-20 15:02:48,733] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,732] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [running]> dependency 'Not In Retry Period' PASSED: True, The context specified that being in a retry period was permitted.
[2020-01-20 15:02:48,733] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,733] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [running]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2020-01-20 15:02:48,734] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,733] {taskinstance.py:672} DEBUG - <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [running]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2020-01-20 15:02:48,734] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,734] {taskinstance.py:655} DEBUG - Dependencies all met for <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [running]>
[2020-01-20 15:02:48,735] {scheduler_job.py:767} DEBUG - Examining active DAG run: <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True>
[2020-01-20 15:02:48,737] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:02:48,750] {logging_mixin.py:112} INFO - [2020-01-20 15:02:48,750] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:31:06+00:00 [failed]> as failed
[2020-01-20 15:02:48,751] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.160 seconds
[2020-01-20 15:03:34,732] {logging_mixin.py:112} INFO - [2020-01-20 15:03:34,731] {settings.py:213} DEBUG - Setting up DB connection pool (PID 7477)
[2020-01-20 15:03:34,737] {scheduler_job.py:153} INFO - Started process (PID=7477) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:03:34,742] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:03:34,743] {logging_mixin.py:112} INFO - [2020-01-20 15:03:34,742] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:03:34,745] {logging_mixin.py:112} INFO - [2020-01-20 15:03:34,745] {dagbag.py:232} DEBUG - Importing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:03:34,957] {logging_mixin.py:112} INFO - [2020-01-20 15:03:34,955] {dagbag.py:370} DEBUG - Loaded DAG <DAG: stream_from_twitter>
[2020-01-20 15:03:34,958] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:03:34,987] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:03:34,997] {scheduler_job.py:691} DEBUG - Dag start date: 2020-01-20T08:00:00+00:00. Next run date: 2020-01-20T09:00:00+00:00
[2020-01-20 15:03:35,000] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True>
[2020-01-20 15:03:35,006] {logging_mixin.py:112} INFO - [2020-01-20 15:03:35,005] {dagrun.py:263} DEBUG - Updating state for <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True> considering 1 task(s)
[2020-01-20 15:03:35,008] {logging_mixin.py:112} INFO - [2020-01-20 15:03:35,008] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:31:06+00:00: manual__2020-01-20T09:31:06+00:00, externally triggered: True> failed
[2020-01-20 15:03:35,010] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:03:35,013] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.276 seconds
[2020-01-20 15:18:18,005] {scheduler_job.py:153} INFO - Started process (PID=7589) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:18:18,014] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:18:18,015] {logging_mixin.py:112} INFO - [2020-01-20 15:18:18,014] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:18:18,092] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:18:18,117] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:18:18,128] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:47:01+00:00: manual__2020-01-20T09:47:01+00:00, externally triggered: True>
[2020-01-20 15:18:18,138] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:18:18,143] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:47:01+00:00 [scheduled]> in ORM
[2020-01-20 15:18:18,151] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.147 seconds
[2020-01-20 15:19:16,249] {scheduler_job.py:153} INFO - Started process (PID=7624) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:19:16,257] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:19:16,257] {logging_mixin.py:112} INFO - [2020-01-20 15:19:16,257] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:19:16,526] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:19:16,560] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:19:16,593] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:47:01+00:00: manual__2020-01-20T09:47:01+00:00, externally triggered: True>
[2020-01-20 15:19:16,613] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:19:16,658] {logging_mixin.py:112} INFO - [2020-01-20 15:19:16,654] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:47:01+00:00 [failed]> as failed
[2020-01-20 15:19:16,659] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.411 seconds
[2020-01-20 15:20:02,383] {scheduler_job.py:153} INFO - Started process (PID=7652) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:20:02,389] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:20:02,390] {logging_mixin.py:112} INFO - [2020-01-20 15:20:02,390] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:20:02,664] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:20:02,689] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:20:02,710] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:47:01+00:00: manual__2020-01-20T09:47:01+00:00, externally triggered: True>
[2020-01-20 15:20:02,726] {logging_mixin.py:112} INFO - [2020-01-20 15:20:02,725] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:47:01+00:00: manual__2020-01-20T09:47:01+00:00, externally triggered: True> failed
[2020-01-20 15:20:02,732] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:20:02,736] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.352 seconds
[2020-01-20 15:20:48,492] {scheduler_job.py:153} INFO - Started process (PID=7679) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:20:48,502] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:20:48,505] {logging_mixin.py:112} INFO - [2020-01-20 15:20:48,504] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:20:48,601] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:20:48,622] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:20:48,634] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:20:48,637] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.144 seconds
[2020-01-20 15:21:34,635] {scheduler_job.py:153} INFO - Started process (PID=7706) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:21:34,642] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:21:34,643] {logging_mixin.py:112} INFO - [2020-01-20 15:21:34,643] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:21:34,907] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:21:34,927] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:21:34,940] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:21:34,943] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.308 seconds
[2020-01-20 15:23:18,213] {scheduler_job.py:153} INFO - Started process (PID=7753) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:23:18,220] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:23:18,220] {logging_mixin.py:112} INFO - [2020-01-20 15:23:18,220] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:23:18,480] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:23:18,500] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:23:18,514] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:23:18,517] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.304 seconds
[2020-01-20 15:24:04,293] {scheduler_job.py:153} INFO - Started process (PID=7791) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:24:04,300] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:24:04,301] {logging_mixin.py:112} INFO - [2020-01-20 15:24:04,301] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:24:04,525] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:24:04,574] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:24:04,586] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:53:52.625293+00:00: manual__2020-01-20T09:53:52.625293+00:00, externally triggered: True>
[2020-01-20 15:24:04,606] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:24:04,620] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:53:52.625293+00:00 [scheduled]> in ORM
[2020-01-20 15:24:04,634] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.342 seconds
[2020-01-20 15:25:03,529] {scheduler_job.py:153} INFO - Started process (PID=7829) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:25:03,536] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:25:03,537] {logging_mixin.py:112} INFO - [2020-01-20 15:25:03,537] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:25:03,658] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:25:03,679] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:25:03,691] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:53:52.625293+00:00: manual__2020-01-20T09:53:52.625293+00:00, externally triggered: True>
[2020-01-20 15:25:03,701] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:25:03,716] {logging_mixin.py:112} INFO - [2020-01-20 15:25:03,716] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:53:52.625293+00:00 [failed]> as failed
[2020-01-20 15:25:03,717] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.188 seconds
[2020-01-20 15:25:49,651] {scheduler_job.py:153} INFO - Started process (PID=7855) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:25:49,658] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:25:49,659] {logging_mixin.py:112} INFO - [2020-01-20 15:25:49,658] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:25:49,739] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:25:49,758] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:25:49,770] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:53:52.625293+00:00: manual__2020-01-20T09:53:52.625293+00:00, externally triggered: True>
[2020-01-20 15:25:49,778] {logging_mixin.py:112} INFO - [2020-01-20 15:25:49,777] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:53:52.625293+00:00: manual__2020-01-20T09:53:52.625293+00:00, externally triggered: True> failed
[2020-01-20 15:25:49,781] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:25:49,784] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.133 seconds
[2020-01-20 15:26:35,787] {scheduler_job.py:153} INFO - Started process (PID=7880) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:26:35,796] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:26:35,797] {logging_mixin.py:112} INFO - [2020-01-20 15:26:35,797] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:26:35,869] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:26:35,888] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:26:35,899] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:26:35,902] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.115 seconds
[2020-01-20 15:27:21,915] {scheduler_job.py:153} INFO - Started process (PID=7907) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:27:21,927] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:27:21,929] {logging_mixin.py:112} INFO - [2020-01-20 15:27:21,928] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:27:22,055] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:27:22,078] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:27:22,091] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:27:22,102] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.188 seconds
[2020-01-20 15:28:08,052] {scheduler_job.py:153} INFO - Started process (PID=7931) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:28:08,060] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:28:08,061] {logging_mixin.py:112} INFO - [2020-01-20 15:28:08,060] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:28:08,166] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:28:08,185] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:28:08,196] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:28:08,199] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.146 seconds
[2020-01-20 15:28:54,186] {scheduler_job.py:153} INFO - Started process (PID=7958) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:28:54,200] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:28:54,203] {logging_mixin.py:112} INFO - [2020-01-20 15:28:54,202] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:28:54,330] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:28:54,353] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:28:54,364] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:28:54,367] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.181 seconds
[2020-01-20 15:29:40,299] {scheduler_job.py:153} INFO - Started process (PID=7985) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:29:40,309] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:29:40,310] {logging_mixin.py:112} INFO - [2020-01-20 15:29:40,310] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:29:40,608] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:29:40,629] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:29:40,641] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:29:40,644] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.345 seconds
[2020-01-20 15:30:26,429] {scheduler_job.py:153} INFO - Started process (PID=8021) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:30:26,437] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:30:26,438] {logging_mixin.py:112} INFO - [2020-01-20 15:30:26,437] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:30:26,539] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:30:26,566] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:30:26,590] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 15:30:26,593] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 15:30:26,601] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:30:26,604] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:00:00+00:00 [scheduled]> in ORM
[2020-01-20 15:30:26,610] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.181 seconds
[2020-01-20 15:31:25,221] {scheduler_job.py:153} INFO - Started process (PID=8054) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:31:25,248] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:31:25,251] {logging_mixin.py:112} INFO - [2020-01-20 15:31:25,249] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:31:25,660] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:31:25,683] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:31:25,695] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 15:31:25,708] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:31:25,723] {logging_mixin.py:112} INFO - [2020-01-20 15:31:25,722] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:00:00+00:00 [failed]> as failed
[2020-01-20 15:31:25,723] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.503 seconds
[2020-01-20 15:32:11,326] {scheduler_job.py:153} INFO - Started process (PID=8080) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:32:11,332] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:32:11,333] {logging_mixin.py:112} INFO - [2020-01-20 15:32:11,332] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:32:11,408] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:32:11,433] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:32:11,445] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 15:32:11,453] {logging_mixin.py:112} INFO - [2020-01-20 15:32:11,453] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False> failed
[2020-01-20 15:32:11,456] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:32:11,458] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.133 seconds
[2020-01-20 15:32:57,454] {scheduler_job.py:153} INFO - Started process (PID=8107) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:32:57,462] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:32:57,463] {logging_mixin.py:112} INFO - [2020-01-20 15:32:57,462] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:32:57,758] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:32:57,787] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:32:57,802] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:32:57,805] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.351 seconds
[2020-01-20 15:33:43,587] {scheduler_job.py:153} INFO - Started process (PID=8136) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:33:43,594] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:33:43,595] {logging_mixin.py:112} INFO - [2020-01-20 15:33:43,595] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:33:43,669] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:33:43,689] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:33:43,700] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:33:43,703] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.116 seconds
[2020-01-20 15:34:29,693] {scheduler_job.py:153} INFO - Started process (PID=8162) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:34:29,700] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:34:29,701] {logging_mixin.py:112} INFO - [2020-01-20 15:34:29,701] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:34:29,991] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:34:30,015] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:34:30,026] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:34:30,029] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.336 seconds
[2020-01-20 15:35:15,820] {scheduler_job.py:153} INFO - Started process (PID=8191) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:35:15,828] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:35:15,829] {logging_mixin.py:112} INFO - [2020-01-20 15:35:15,829] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:35:15,913] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:35:15,935] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:35:15,946] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:35:15,949] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.129 seconds
[2020-01-20 15:36:01,959] {scheduler_job.py:153} INFO - Started process (PID=8215) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:36:01,968] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:36:01,969] {logging_mixin.py:112} INFO - [2020-01-20 15:36:01,968] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:36:02,044] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:36:02,063] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:36:02,075] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:36:02,078] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.118 seconds
[2020-01-20 15:36:48,099] {scheduler_job.py:153} INFO - Started process (PID=8240) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:36:48,106] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:36:48,107] {logging_mixin.py:112} INFO - [2020-01-20 15:36:48,107] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:36:48,187] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:36:48,208] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:36:48,221] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:36:48,223] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.124 seconds
[2020-01-20 15:37:34,212] {scheduler_job.py:153} INFO - Started process (PID=8274) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:37:34,223] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:37:34,224] {logging_mixin.py:112} INFO - [2020-01-20 15:37:34,224] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:37:34,304] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:37:34,324] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:37:34,336] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:37:34,339] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.127 seconds
[2020-01-20 15:38:20,329] {scheduler_job.py:153} INFO - Started process (PID=8310) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:38:20,338] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:38:20,339] {logging_mixin.py:112} INFO - [2020-01-20 15:38:20,338] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:38:20,353] {logging_mixin.py:112} INFO - [2020-01-20 15:38:20,346] {dagbag.py:246} ERROR - Failed to import: /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
Traceback (most recent call last):
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/site-packages/airflow/models/dagbag.py", line 243, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/imp.py", line 171, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 696, in _load
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py", line 1, in <module>
    from bot.twitter_stream import read_stream_of_tweets
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/bot/twitter_stream.py", line 57
    if __name__ == "__main__":
     ^
IndentationError: expected an indented block
[2020-01-20 15:38:20,353] {scheduler_job.py:1553} WARNING - No viable dags retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:38:20,366] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.037 seconds
[2020-01-20 15:39:06,445] {scheduler_job.py:153} INFO - Started process (PID=8345) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:39:06,453] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:39:06,454] {logging_mixin.py:112} INFO - [2020-01-20 15:39:06,453] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:39:06,556] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:39:06,585] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:39:06,596] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:39:06,601] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.156 seconds
[2020-01-20 15:44:27,890] {scheduler_job.py:153} INFO - Started process (PID=8435) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:44:27,896] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:44:27,897] {logging_mixin.py:112} INFO - [2020-01-20 15:44:27,897] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:44:28,097] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:44:28,117] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:44:28,128] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:14:15.703793+00:00: manual__2020-01-20T10:14:15.703793+00:00, externally triggered: True>
[2020-01-20 15:44:28,139] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:44:28,142] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:14:15.703793+00:00 [scheduled]> in ORM
[2020-01-20 15:44:28,149] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.259 seconds
[2020-01-20 15:45:32,554] {scheduler_job.py:153} INFO - Started process (PID=8467) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:45:32,564] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:45:32,565] {logging_mixin.py:112} INFO - [2020-01-20 15:45:32,564] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:45:32,779] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:45:32,803] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:45:32,814] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:14:15.703793+00:00: manual__2020-01-20T10:14:15.703793+00:00, externally triggered: True>
[2020-01-20 15:45:32,822] {logging_mixin.py:112} INFO - [2020-01-20 15:45:32,822] {dagrun.py:318} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 10:14:15.703793+00:00: manual__2020-01-20T10:14:15.703793+00:00, externally triggered: True> successful
[2020-01-20 15:45:32,827] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:45:32,831] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.277 seconds
[2020-01-20 15:46:18,671] {scheduler_job.py:153} INFO - Started process (PID=8494) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:46:18,677] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:46:18,678] {logging_mixin.py:112} INFO - [2020-01-20 15:46:18,678] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:46:18,756] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:46:18,776] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:46:18,787] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:46:18,790] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.120 seconds
[2020-01-20 15:47:04,808] {scheduler_job.py:153} INFO - Started process (PID=8525) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:47:04,816] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:47:04,817] {logging_mixin.py:112} INFO - [2020-01-20 15:47:04,817] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:47:04,910] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:47:04,935] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:47:04,948] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:47:04,952] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.144 seconds
[2020-01-20 15:47:50,920] {scheduler_job.py:153} INFO - Started process (PID=8557) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:47:50,928] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:47:50,929] {logging_mixin.py:112} INFO - [2020-01-20 15:47:50,928] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:47:51,026] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:47:51,050] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:47:51,065] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:47:51,067] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.148 seconds
[2020-01-20 15:48:37,048] {scheduler_job.py:153} INFO - Started process (PID=8585) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:48:37,056] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:48:37,057] {logging_mixin.py:112} INFO - [2020-01-20 15:48:37,057] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:48:37,133] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:48:37,153] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:48:37,164] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:48:37,167] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.119 seconds
[2020-01-20 15:49:23,198] {scheduler_job.py:153} INFO - Started process (PID=8609) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:49:23,207] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:49:23,208] {logging_mixin.py:112} INFO - [2020-01-20 15:49:23,207] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:49:23,282] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:49:23,303] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:49:23,315] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:49:23,317] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.119 seconds
[2020-01-20 15:50:09,307] {scheduler_job.py:153} INFO - Started process (PID=8639) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:50:09,314] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:50:09,316] {logging_mixin.py:112} INFO - [2020-01-20 15:50:09,315] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:50:09,338] {logging_mixin.py:112} INFO - [2020-01-20 15:50:09,322] {dagbag.py:246} ERROR - Failed to import: /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
Traceback (most recent call last):
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/site-packages/airflow/models/dagbag.py", line 243, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/venv_datascience/lib/python3.7/imp.py", line 171, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 696, in _load
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py", line 1, in <module>
    from bot.twitter_stream import read_stream_of_tweets
  File "/Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/bot/twitter_stream.py", line 68
    if __name__ == "__main__":
     ^
IndentationError: expected an indented block
[2020-01-20 15:50:09,338] {scheduler_job.py:1553} WARNING - No viable dags retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:50:09,351] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.044 seconds
[2020-01-20 15:50:55,433] {scheduler_job.py:153} INFO - Started process (PID=8682) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:50:55,439] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:50:55,440] {logging_mixin.py:112} INFO - [2020-01-20 15:50:55,440] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:50:55,518] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:50:55,539] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:50:55,551] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:50:55,555] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.122 seconds
[2020-01-20 15:51:41,549] {scheduler_job.py:153} INFO - Started process (PID=8719) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:51:41,557] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:51:41,558] {logging_mixin.py:112} INFO - [2020-01-20 15:51:41,557] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:51:41,642] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:51:41,665] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:51:41,676] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:51:41,679] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.130 seconds
[2020-01-20 15:52:27,660] {scheduler_job.py:153} INFO - Started process (PID=8755) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:52:27,673] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:52:27,674] {logging_mixin.py:112} INFO - [2020-01-20 15:52:27,674] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:52:27,986] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:52:28,010] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:52:28,021] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:52:28,024] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.364 seconds
[2020-01-20 15:53:13,791] {scheduler_job.py:153} INFO - Started process (PID=8793) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:53:13,797] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:53:13,798] {logging_mixin.py:112} INFO - [2020-01-20 15:53:13,798] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:53:14,026] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:53:14,049] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:53:14,065] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:53:14,067] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.277 seconds
[2020-01-20 15:53:59,916] {scheduler_job.py:153} INFO - Started process (PID=8821) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:53:59,924] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:53:59,925] {logging_mixin.py:112} INFO - [2020-01-20 15:53:59,925] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:54:00,029] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:54:00,057] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:54:00,072] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:54:00,076] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.161 seconds
[2020-01-20 15:54:46,042] {scheduler_job.py:153} INFO - Started process (PID=8846) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:54:46,051] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:54:46,052] {logging_mixin.py:112} INFO - [2020-01-20 15:54:46,052] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:54:46,197] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:54:46,221] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:54:46,233] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:54:46,236] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.194 seconds
[2020-01-20 15:55:32,168] {scheduler_job.py:153} INFO - Started process (PID=8871) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:55:32,176] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:55:32,177] {logging_mixin.py:112} INFO - [2020-01-20 15:55:32,176] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:55:32,302] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:55:32,324] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:55:32,336] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:55:32,339] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.171 seconds
[2020-01-20 15:56:18,288] {scheduler_job.py:153} INFO - Started process (PID=8897) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:56:18,296] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:56:18,297] {logging_mixin.py:112} INFO - [2020-01-20 15:56:18,297] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:56:18,398] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:56:18,419] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:56:18,430] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:56:18,433] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.145 seconds
[2020-01-20 15:57:04,427] {scheduler_job.py:153} INFO - Started process (PID=8923) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:57:04,435] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:57:04,436] {logging_mixin.py:112} INFO - [2020-01-20 15:57:04,436] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:57:04,525] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:57:04,545] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:57:04,558] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:57:04,561] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.134 seconds
[2020-01-20 15:57:50,570] {scheduler_job.py:153} INFO - Started process (PID=8950) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:57:50,576] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:57:50,577] {logging_mixin.py:112} INFO - [2020-01-20 15:57:50,577] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:57:50,697] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:57:50,719] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:57:50,733] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:57:50,736] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.166 seconds
[2020-01-20 15:58:36,717] {scheduler_job.py:153} INFO - Started process (PID=8976) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:58:36,725] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:58:36,726] {logging_mixin.py:112} INFO - [2020-01-20 15:58:36,726] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:58:36,966] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:58:36,986] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:58:36,998] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:58:37,000] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.284 seconds
[2020-01-20 15:59:22,828] {scheduler_job.py:153} INFO - Started process (PID=9004) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:59:22,835] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 15:59:22,836] {logging_mixin.py:112} INFO - [2020-01-20 15:59:22,836] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:59:22,923] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 15:59:22,948] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 15:59:22,975] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 15:59:22,979] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.152 seconds
[2020-01-20 16:00:08,927] {scheduler_job.py:153} INFO - Started process (PID=9035) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:00:08,933] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:00:08,934] {logging_mixin.py:112} INFO - [2020-01-20 16:00:08,934] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:00:09,398] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:00:09,421] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:00:09,434] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:00:09,437] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.509 seconds
[2020-01-20 16:00:55,070] {scheduler_job.py:153} INFO - Started process (PID=9073) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:00:55,076] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:00:55,076] {logging_mixin.py:112} INFO - [2020-01-20 16:00:55,076] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:00:55,176] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:00:55,198] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:00:55,213] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:00:55,216] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.146 seconds
[2020-01-20 16:01:41,194] {scheduler_job.py:153} INFO - Started process (PID=9101) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:01:41,205] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:01:41,206] {logging_mixin.py:112} INFO - [2020-01-20 16:01:41,206] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:01:41,497] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:01:41,522] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:01:41,535] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:01:41,537] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.343 seconds
[2020-01-20 16:02:27,297] {scheduler_job.py:153} INFO - Started process (PID=9142) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:02:27,305] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:02:27,306] {logging_mixin.py:112} INFO - [2020-01-20 16:02:27,306] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:02:27,408] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:02:27,429] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:02:27,444] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:02:27,447] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.150 seconds
[2020-01-20 16:03:13,427] {scheduler_job.py:153} INFO - Started process (PID=9176) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:03:13,437] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:03:13,438] {logging_mixin.py:112} INFO - [2020-01-20 16:03:13,438] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:03:13,743] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:03:13,769] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:03:13,784] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:03:13,786] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.359 seconds
[2020-01-20 16:03:59,562] {scheduler_job.py:153} INFO - Started process (PID=9202) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:03:59,569] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:03:59,570] {logging_mixin.py:112} INFO - [2020-01-20 16:03:59,569] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:03:59,871] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:03:59,893] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:03:59,911] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:33:23.212102+00:00: manual__2020-01-20T10:33:23.212102+00:00, externally triggered: True>
[2020-01-20 16:03:59,935] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:03:59,945] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:33:23.212102+00:00 [scheduled]> in ORM
[2020-01-20 16:03:59,958] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.396 seconds
[2020-01-20 16:04:58,443] {scheduler_job.py:153} INFO - Started process (PID=9247) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:04:58,452] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:04:58,453] {logging_mixin.py:112} INFO - [2020-01-20 16:04:58,452] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:04:58,542] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:04:58,563] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:04:58,578] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:33:23.212102+00:00: manual__2020-01-20T10:33:23.212102+00:00, externally triggered: True>
[2020-01-20 16:04:58,588] {logging_mixin.py:112} INFO - [2020-01-20 16:04:58,588] {dagrun.py:318} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 10:33:23.212102+00:00: manual__2020-01-20T10:33:23.212102+00:00, externally triggered: True> successful
[2020-01-20 16:04:58,593] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:04:58,596] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.153 seconds
[2020-01-20 16:05:44,552] {scheduler_job.py:153} INFO - Started process (PID=9273) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:05:44,561] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:05:44,562] {logging_mixin.py:112} INFO - [2020-01-20 16:05:44,562] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:05:44,645] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:05:44,664] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:05:44,676] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:05:44,678] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.126 seconds
[2020-01-20 16:06:30,712] {scheduler_job.py:153} INFO - Started process (PID=9299) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:06:30,720] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:06:30,721] {logging_mixin.py:112} INFO - [2020-01-20 16:06:30,720] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:06:30,798] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:06:30,821] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:06:30,833] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:06:30,836] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.124 seconds
[2020-01-20 16:07:16,849] {scheduler_job.py:153} INFO - Started process (PID=9323) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:07:16,856] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:07:16,857] {logging_mixin.py:112} INFO - [2020-01-20 16:07:16,856] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:07:17,063] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:07:17,086] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:07:17,115] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 16:07:17,119] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 16:07:17,124] {logging_mixin.py:112} INFO - [2020-01-20 16:07:17,124] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False> failed
[2020-01-20 16:07:17,127] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:07:17,130] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.281 seconds
[2020-01-20 16:08:02,963] {scheduler_job.py:153} INFO - Started process (PID=9366) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:08:02,971] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:08:02,972] {logging_mixin.py:112} INFO - [2020-01-20 16:08:02,972] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:08:03,055] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:08:03,076] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:08:03,088] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:08:03,091] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.128 seconds
[2020-01-20 16:08:49,118] {scheduler_job.py:153} INFO - Started process (PID=9391) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:08:49,127] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:08:49,127] {logging_mixin.py:112} INFO - [2020-01-20 16:08:49,127] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:08:49,206] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:08:49,226] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:08:49,238] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:08:49,241] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.122 seconds
[2020-01-20 16:09:35,258] {scheduler_job.py:153} INFO - Started process (PID=9422) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:09:35,267] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:09:35,268] {logging_mixin.py:112} INFO - [2020-01-20 16:09:35,267] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:09:35,346] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:09:35,368] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:09:35,379] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:09:35,382] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.124 seconds
[2020-01-20 16:10:21,372] {scheduler_job.py:153} INFO - Started process (PID=9451) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:10:21,379] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:10:21,380] {logging_mixin.py:112} INFO - [2020-01-20 16:10:21,380] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:10:21,615] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:10:21,637] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:10:21,648] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:10:21,651] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.279 seconds
[2020-01-20 16:11:07,492] {scheduler_job.py:153} INFO - Started process (PID=10442) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:11:07,500] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:11:07,501] {logging_mixin.py:112} INFO - [2020-01-20 16:11:07,500] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:11:07,706] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:11:07,753] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:11:07,780] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:11:07,784] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.292 seconds
[2020-01-20 16:13:13,494] {scheduler_job.py:153} INFO - Started process (PID=10510) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:13:13,502] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:13:13,503] {logging_mixin.py:112} INFO - [2020-01-20 16:13:13,503] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:13:13,810] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:13:13,834] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:13:13,855] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 16:13:13,858] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 16:13:13,864] {logging_mixin.py:112} INFO - [2020-01-20 16:13:13,863] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False> failed
[2020-01-20 16:13:13,867] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:13:13,870] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.376 seconds
[2020-01-20 16:14:12,499] {scheduler_job.py:153} INFO - Started process (PID=10545) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:14:12,509] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:14:12,510] {logging_mixin.py:112} INFO - [2020-01-20 16:14:12,509] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:14:12,814] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:14:12,833] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:14:12,845] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:14:12,848] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.349 seconds
[2020-01-20 16:14:58,641] {scheduler_job.py:153} INFO - Started process (PID=10572) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:14:58,656] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:14:58,659] {logging_mixin.py:112} INFO - [2020-01-20 16:14:58,658] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:14:58,895] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:14:58,915] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:14:58,928] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:14:58,931] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.290 seconds
[2020-01-20 16:15:44,764] {scheduler_job.py:153} INFO - Started process (PID=10607) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:15:44,773] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:15:44,774] {logging_mixin.py:112} INFO - [2020-01-20 16:15:44,773] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:15:45,126] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:15:45,150] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:15:45,165] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:15:45,169] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.404 seconds
[2020-01-20 16:16:30,880] {scheduler_job.py:153} INFO - Started process (PID=10637) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:16:30,889] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:16:30,890] {logging_mixin.py:112} INFO - [2020-01-20 16:16:30,890] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:16:31,386] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:16:31,407] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:16:31,418] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:16:31,423] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.543 seconds
[2020-01-20 16:17:17,011] {scheduler_job.py:153} INFO - Started process (PID=10665) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:17:17,018] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:17:17,020] {logging_mixin.py:112} INFO - [2020-01-20 16:17:17,019] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:17:17,117] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:17:17,138] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:17:17,149] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:17:17,152] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.141 seconds
[2020-01-20 16:18:03,133] {scheduler_job.py:153} INFO - Started process (PID=10698) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:18:03,140] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:18:03,141] {logging_mixin.py:112} INFO - [2020-01-20 16:18:03,141] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:18:03,228] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:18:03,250] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:18:03,262] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:18:03,265] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.131 seconds
[2020-01-20 16:18:49,261] {scheduler_job.py:153} INFO - Started process (PID=10725) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:18:49,267] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:18:49,268] {logging_mixin.py:112} INFO - [2020-01-20 16:18:49,268] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:18:49,344] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:18:49,371] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:18:49,390] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:18:49,392] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.131 seconds
[2020-01-20 16:19:35,372] {scheduler_job.py:153} INFO - Started process (PID=10750) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:19:35,381] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:19:35,381] {logging_mixin.py:112} INFO - [2020-01-20 16:19:35,381] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:19:35,484] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:19:35,509] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:19:35,521] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:19:35,524] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.152 seconds
[2020-01-20 16:20:21,487] {scheduler_job.py:153} INFO - Started process (PID=10776) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:20:21,494] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:20:21,495] {logging_mixin.py:112} INFO - [2020-01-20 16:20:21,495] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:20:21,822] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:20:21,847] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:20:21,860] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:20:21,864] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.376 seconds
[2020-01-20 16:21:20,284] {scheduler_job.py:153} INFO - Started process (PID=10813) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:21:20,297] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:21:20,298] {logging_mixin.py:112} INFO - [2020-01-20 16:21:20,298] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:21:20,576] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:21:20,605] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:21:20,618] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:21:20,623] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.339 seconds
[2020-01-20 16:22:06,388] {scheduler_job.py:153} INFO - Started process (PID=10839) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:22:06,396] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:22:06,397] {logging_mixin.py:112} INFO - [2020-01-20 16:22:06,396] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:22:06,488] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:22:06,516] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:22:06,530] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:22:06,533] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.145 seconds
[2020-01-20 16:22:52,506] {scheduler_job.py:153} INFO - Started process (PID=10868) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:22:52,513] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:22:52,514] {logging_mixin.py:112} INFO - [2020-01-20 16:22:52,514] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:22:52,587] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:22:52,611] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:22:52,623] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:22:52,625] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.120 seconds
[2020-01-20 16:23:38,631] {scheduler_job.py:153} INFO - Started process (PID=10896) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:23:38,641] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:23:38,642] {logging_mixin.py:112} INFO - [2020-01-20 16:23:38,642] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:23:38,721] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:23:38,740] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:23:38,753] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:23:38,756] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.124 seconds
[2020-01-20 16:24:24,763] {scheduler_job.py:153} INFO - Started process (PID=10926) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:24:24,769] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:24:24,770] {logging_mixin.py:112} INFO - [2020-01-20 16:24:24,770] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:24:25,026] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:24:25,047] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:24:25,064] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:24:25,068] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.305 seconds
[2020-01-20 16:26:51,917] {scheduler_job.py:153} INFO - Started process (PID=10987) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:26:51,923] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:26:51,924] {logging_mixin.py:112} INFO - [2020-01-20 16:26:51,924] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:26:52,004] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:26:52,023] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:26:52,034] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:56:39.993587+00:00: manual__2020-01-20T10:56:39.993587+00:00, externally triggered: True>
[2020-01-20 16:26:52,044] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:26:52,047] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:56:39.993587+00:00 [scheduled]> in ORM
[2020-01-20 16:26:52,054] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.137 seconds
[2020-01-20 16:27:50,057] {scheduler_job.py:153} INFO - Started process (PID=11022) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:27:50,064] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:27:50,065] {logging_mixin.py:112} INFO - [2020-01-20 16:27:50,064] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:27:50,156] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:27:50,175] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:27:50,188] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:56:39.993587+00:00: manual__2020-01-20T10:56:39.993587+00:00, externally triggered: True>
[2020-01-20 16:27:50,199] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:27:50,211] {logging_mixin.py:112} INFO - [2020-01-20 16:27:50,210] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:56:39.993587+00:00 [failed]> as failed
[2020-01-20 16:27:50,211] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.155 seconds
[2020-01-20 16:28:36,184] {scheduler_job.py:153} INFO - Started process (PID=11060) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:28:36,191] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:28:36,192] {logging_mixin.py:112} INFO - [2020-01-20 16:28:36,192] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:28:36,298] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:28:36,320] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:28:36,335] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:56:39.993587+00:00: manual__2020-01-20T10:56:39.993587+00:00, externally triggered: True>
[2020-01-20 16:28:36,345] {logging_mixin.py:112} INFO - [2020-01-20 16:28:36,344] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 10:56:39.993587+00:00: manual__2020-01-20T10:56:39.993587+00:00, externally triggered: True> failed
[2020-01-20 16:28:36,348] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:28:36,351] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.167 seconds
[2020-01-20 16:29:22,328] {scheduler_job.py:153} INFO - Started process (PID=11102) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:29:22,335] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:29:22,336] {logging_mixin.py:112} INFO - [2020-01-20 16:29:22,336] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:29:22,492] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:29:22,545] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:29:22,581] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:29:22,593] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.264 seconds
[2020-01-20 16:30:08,453] {scheduler_job.py:153} INFO - Started process (PID=11131) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:30:08,461] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:30:08,462] {logging_mixin.py:112} INFO - [2020-01-20 16:30:08,462] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:30:08,574] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:30:08,620] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:30:08,722] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:30:08,737] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 09:00:00+00:00: scheduled__2020-01-20T09:00:00+00:00, externally triggered: False>
[2020-01-20 16:30:08,749] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:30:08,761] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:56:39.993587+00:00: manual__2020-01-20T10:56:39.993587+00:00, externally triggered: True>
[2020-01-20 16:30:08,796] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:30:08,804] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 09:00:00+00:00 [scheduled]> in ORM
[2020-01-20 16:30:08,811] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:00:00+00:00 [scheduled]> in ORM
[2020-01-20 16:30:08,819] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:56:39.993587+00:00 [scheduled]> in ORM
[2020-01-20 16:30:08,836] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.383 seconds
[2020-01-20 16:31:34,149] {scheduler_job.py:153} INFO - Started process (PID=11172) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:31:34,157] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:31:34,158] {logging_mixin.py:112} INFO - [2020-01-20 16:31:34,158] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:31:34,451] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:31:34,482] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:31:34,504] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:31:34,507] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:31:34,517] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:31:34,520] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:00:00+00:00 [scheduled]> in ORM
[2020-01-20 16:31:34,527] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.379 seconds
[2020-01-20 16:32:32,658] {scheduler_job.py:153} INFO - Started process (PID=11207) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:32:32,667] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:32:32,668] {logging_mixin.py:112} INFO - [2020-01-20 16:32:32,668] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:32:32,765] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:32:32,774] {logging_mixin.py:112} INFO - [2020-01-20 16:32:32,773] {dag.py:1376} INFO - Creating ORM DAG for stream_from_twitter
[2020-01-20 16:32:32,787] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.129 seconds
[2020-01-20 16:33:31,419] {scheduler_job.py:153} INFO - Started process (PID=11251) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:33:31,427] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:33:31,428] {logging_mixin.py:112} INFO - [2020-01-20 16:33:31,428] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:33:31,633] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:33:31,651] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:33:31,676] {scheduler_job.py:1272} INFO - Created <DagRun stream_from_twitter @ 2020-01-20T10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:33:31,678] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:33:31,684] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 11:03:18.210557+00:00: manual__2020-01-20T11:03:18.210557+00:00, externally triggered: True>
[2020-01-20 16:33:31,694] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:33:31,697] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:00:00+00:00 [scheduled]> in ORM
[2020-01-20 16:33:31,701] {scheduler_job.py:1613} INFO - Creating / updating <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 11:03:18.210557+00:00 [scheduled]> in ORM
[2020-01-20 16:33:31,708] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.289 seconds
[2020-01-20 16:34:41,085] {scheduler_job.py:153} INFO - Started process (PID=11286) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:34:41,092] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:34:41,093] {logging_mixin.py:112} INFO - [2020-01-20 16:34:41,093] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:34:41,170] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:34:41,192] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:34:41,203] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:34:41,210] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 11:03:18.210557+00:00: manual__2020-01-20T11:03:18.210557+00:00, externally triggered: True>
[2020-01-20 16:34:41,221] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:34:41,242] {logging_mixin.py:112} INFO - [2020-01-20 16:34:41,241] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 11:03:18.210557+00:00 [failed]> as failed
[2020-01-20 16:34:41,252] {logging_mixin.py:112} INFO - [2020-01-20 16:34:41,252] {dagbag.py:343} INFO - Marked zombie job <TaskInstance: stream_from_twitter.stream_from_twitter_to_kafka 2020-01-20 10:00:00+00:00 [failed]> as failed
[2020-01-20 16:34:41,252] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.167 seconds
[2020-01-20 16:35:27,211] {scheduler_job.py:153} INFO - Started process (PID=11313) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:35:27,217] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:35:27,218] {logging_mixin.py:112} INFO - [2020-01-20 16:35:27,218] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:35:27,291] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:35:27,318] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:35:27,329] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False>
[2020-01-20 16:35:27,336] {logging_mixin.py:112} INFO - [2020-01-20 16:35:27,336] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 10:00:00+00:00: scheduled__2020-01-20T10:00:00+00:00, externally triggered: False> failed
[2020-01-20 16:35:27,339] {scheduler_job.py:740} INFO - Examining DAG run <DagRun stream_from_twitter @ 2020-01-20 11:03:18.210557+00:00: manual__2020-01-20T11:03:18.210557+00:00, externally triggered: True>
[2020-01-20 16:35:27,345] {logging_mixin.py:112} INFO - [2020-01-20 16:35:27,345] {dagrun.py:309} INFO - Marking run <DagRun stream_from_twitter @ 2020-01-20 11:03:18.210557+00:00: manual__2020-01-20T11:03:18.210557+00:00, externally triggered: True> failed
[2020-01-20 16:35:27,348] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:35:27,351] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.139 seconds
[2020-01-20 16:36:13,349] {scheduler_job.py:153} INFO - Started process (PID=11339) to work on /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:36:13,357] {scheduler_job.py:1539} INFO - Processing file /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py for tasks to queue
[2020-01-20 16:36:13,358] {logging_mixin.py:112} INFO - [2020-01-20 16:36:13,358] {dagbag.py:403} INFO - Filling up the DagBag from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:36:13,452] {scheduler_job.py:1551} INFO - DAG(s) dict_keys(['stream_from_twitter']) retrieved from /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py
[2020-01-20 16:36:13,475] {scheduler_job.py:1262} INFO - Processing stream_from_twitter
[2020-01-20 16:36:13,489] {scheduler_job.py:440} INFO - Skipping SLA check for <DAG: stream_from_twitter> because no tasks in DAG have SLAs
[2020-01-20 16:36:13,492] {scheduler_job.py:161} INFO - Processing /Users/aneeshmakala/Documents/ComputerScience/datascience/hapPy/airflow/dags/stream_from_twitter.py took 0.143 seconds
